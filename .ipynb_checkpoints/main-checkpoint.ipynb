{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wget\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "county_data = pd.read_csv(\"us-counties.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = county_data[county_data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls.to_csv(\"nulls.csv\")\n",
    "# All rows where FIPS is null. This consists of (a) New York City (all reported as one unit), (b) various Unknowns. \n",
    "# See NYT documentation on geographical exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 20200201\n",
      "Downloading 20200202\n",
      "Downloading 20200203\n",
      "Downloading 20200204\n",
      "Downloading 20200205\n",
      "Downloading 20200206\n",
      "Downloading 20200207\n",
      "Downloading 20200208\n",
      "Downloading 20200209\n",
      "Downloading 20200210\n",
      "Downloading 20200211\n",
      "Downloading 20200212\n",
      "Downloading 20200213\n",
      "Downloading 20200214\n",
      "Downloading 20200215\n",
      "Downloading 20200216\n",
      "Downloading 20200217\n",
      "Downloading 20200218\n",
      "Downloading 20200219\n",
      "Downloading 20200220\n",
      "Downloading 20200221\n",
      "Downloading 20200222\n",
      "Downloading 20200223\n",
      "Downloading 20200224\n",
      "Downloading 20200225\n",
      "Downloading 20200226\n",
      "Downloading 20200227\n",
      "Downloading 20200228\n",
      "Downloading 20200229\n",
      "No file for date 20200230\n",
      "No file for date 20200231\n",
      "Downloading 20200301\n",
      "Downloading 20200302\n",
      "Downloading 20200303\n",
      "Downloading 20200304\n",
      "Downloading 20200305\n",
      "Downloading 20200306\n",
      "Downloading 20200307\n",
      "Downloading 20200308\n",
      "Downloading 20200309\n",
      "Downloading 20200310\n",
      "Downloading 20200311\n",
      "Downloading 20200312\n",
      "Downloading 20200313\n",
      "Downloading 20200314\n",
      "Downloading 20200315\n",
      "Downloading 20200316\n",
      "Downloading 20200317\n",
      "Downloading 20200318\n",
      "Downloading 20200319\n",
      "Downloading 20200320\n",
      "Downloading 20200321\n",
      "Downloading 20200322\n",
      "Downloading 20200323\n",
      "Downloading 20200324\n",
      "Downloading 20200325\n",
      "Downloading 20200326\n",
      "Downloading 20200327\n",
      "Downloading 20200328\n",
      "Downloading 20200329\n",
      "Downloading 20200330\n",
      "Downloading 20200331\n",
      "Downloading 20200401\n",
      "Downloading 20200402\n",
      "Downloading 20200403\n",
      "Downloading 20200404\n",
      "Downloading 20200405\n",
      "Downloading 20200406\n",
      "Downloading 20200407\n",
      "Downloading 20200408\n",
      "Downloading 20200409\n",
      "Downloading 20200410\n",
      "Downloading 20200411\n",
      "Downloading 20200412\n",
      "Downloading 20200413\n",
      "Downloading 20200414\n",
      "Downloading 20200415\n",
      "Downloading 20200416\n",
      "Downloading 20200417\n",
      "Downloading 20200418\n",
      "Downloading 20200419\n",
      "Downloading 20200420\n",
      "Downloading 20200421\n",
      "Downloading 20200422\n",
      "Downloading 20200423\n",
      "Downloading 20200424\n",
      "Downloading 20200425\n",
      "Downloading 20200426\n",
      "Downloading 20200427\n",
      "Downloading 20200428\n",
      "Downloading 20200429\n",
      "Downloading 20200430\n",
      "No file for date 20200431\n",
      "Downloading 20200501\n",
      "Downloading 20200502\n",
      "Downloading 20200503\n",
      "Downloading 20200504\n",
      "Downloading 20200505\n",
      "Downloading 20200506\n",
      "Downloading 20200507\n",
      "Downloading 20200508\n",
      "Downloading 20200509\n",
      "Downloading 20200510\n",
      "Downloading 20200511\n",
      "Downloading 20200512\n",
      "Downloading 20200513\n",
      "Downloading 20200514\n",
      "Downloading 20200515\n",
      "Downloading 20200516\n",
      "Downloading 20200517\n",
      "Downloading 20200518\n",
      "Downloading 20200519\n",
      "Downloading 20200520\n",
      "No file for date 20200521\n",
      "No file for date 20200522\n",
      "No file for date 20200523\n",
      "No file for date 20200524\n",
      "No file for date 20200525\n",
      "No file for date 20200526\n",
      "No file for date 20200527\n",
      "No file for date 20200528\n",
      "No file for date 20200529\n",
      "No file for date 20200530\n",
      "No file for date 20200531\n"
     ]
    }
   ],
   "source": [
    "# Download daily data files -- did this on 5/20. Saved as .txt files in /data\n",
    "\n",
    "for m in range(4):\n",
    "    for d in range(31):\n",
    "        month = str(m+2).zfill(2)\n",
    "        day = str(d+1).zfill(2)\n",
    "        date = \"2020\"+month+day\n",
    "        url = \"https://s3-us-west-1.amazonaws.com//files.airnowtech.org/airnow/2020/\"+date+\"/daily_data_v2.dat\"\n",
    "        try:\n",
    "            wget.download(url, \"./data/\"+date+\".txt\")\n",
    "            print('Downloading', date)\n",
    "        except:\n",
    "            print('No file for date', date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make DFs from .txt files\n",
    "# date_dfs is a dictionary of DFs where keys are dates in MMDD format\n",
    "\n",
    "# DF column values: \"aqsid\" = site ID, first 5 digits are FIPS code; \n",
    "# \"period\" = averaging period (either peak 1-hr, peak 8-hr average, or 24-hr average);\n",
    "# \"aqi_value\" = 8-hr or 24-hr AQI value, -999 = peak 1-hr ozone or peak 24-hr SO2; \"aqi_category\" = 0=good etc.;\n",
    "# \"aqsid_full\" = AQS site ID preceded by 3-digit country code\n",
    "\n",
    "date_dfs = {}\n",
    "allfiles = os.listdir('data')[1:]\n",
    "for filename in allfiles:\n",
    "    date = filename[4:8]\n",
    "    df = pd.read_csv('data/'+filename, sep=\"|\", header=None)\n",
    "    df.columns = [\"date\",\"aqsid\",\"sitename\",\"parameter\",\"units\",\"value\",\"period\",\"source\",\"aqi_value\",\"aqi_category\",\"latitude\",\"longitude\",\"aqsid_full\"]\n",
    "    df['fips'] = df['aqsid'].str.slice(stop=5)  # add FIPS column based on first 5 digits of AQSID\n",
    "    del df['aqsid']\n",
    "    del df['source']\n",
    "    del df['latitude']\n",
    "    del df['longitude']\n",
    "    # del df['aqsid_full'] -- no, this might tell you if it's a non-US site?\n",
    "    # delete non-US rows based on this?\n",
    "    df.to_csv('air_quality_tables/'+filename[:-3]+'csv')\n",
    "    date_dfs[date] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dfs['0520'] = None   # dropped today's table because it contained data from 5/18 for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of FIPS codes to county/state\n",
    "\n",
    "fips = pd.read_html('https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = fips.drop(3232) # last row was gibberish\n",
    "# did some other massaging which I deleted the code for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips.to_csv('fips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = pd.read_csv('fips.csv', dtype={'fips':\"str\"})\n",
    "del fips['Unnamed: 0']\n",
    "\n",
    "# for some reason the CSV loses leading zeroes in the fips column; \n",
    "# opening like this fixes it for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date format is different in county_data and in air quality data tables\n",
    "# remove non-US data, based on first 3 digits of full AQSID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
